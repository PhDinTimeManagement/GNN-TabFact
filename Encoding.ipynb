{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model_type = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_type)\n",
    "\n",
    "BERT = BertModel.from_pretrained(model_type)\n",
    "BERT.to(device)\n",
    "\n",
    "f = '1-10006830-1.html.csv'\n",
    "\n",
    "table = pandas.read_csv('all_csv/{}'.format(f), '#')\n",
    "cols = table.columns\n",
    "\n",
    "representation = torch.FloatTensor(len(table), len(cols), 768).to(device)\n",
    "for i in range(len(table)):\n",
    "    entry = table.iloc[i]\n",
    "    for j, col in enumerate(cols):\n",
    "        text = '[CLS] in row {}, {} is {}'.format(i+1, col, entry[col])\n",
    "        inp = torch.tensor([tokenizer.encode(text)]).to(device)\n",
    "        encoded = BERT(inp)[1][0]\n",
    "        representation[i][j] = encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aircraft</th>\n",
       "      <th>description</th>\n",
       "      <th>max gross weight</th>\n",
       "      <th>total disk area</th>\n",
       "      <th>max disk loading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>robinson r - 22</td>\n",
       "      <td>light utility helicopter</td>\n",
       "      <td>1370 lb (635 kg)</td>\n",
       "      <td>497 ft square (46.2 m square)</td>\n",
       "      <td>2.6 lb / ft square (14 kg / m square)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>bell 206b3 jetranger</td>\n",
       "      <td>turboshaft utility helicopter</td>\n",
       "      <td>3200 lb (1451 kg)</td>\n",
       "      <td>872 ft square (81.1 m square)</td>\n",
       "      <td>3.7 lb / ft square (18 kg / m square)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ch - 47d chinook</td>\n",
       "      <td>tandem rotor helicopter</td>\n",
       "      <td>50000 lb (22680 kg)</td>\n",
       "      <td>5655 ft square (526 m square)</td>\n",
       "      <td>8.8 lb / ft square (43 kg / m square)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>mil mi - 26</td>\n",
       "      <td>heavy - lift helicopter</td>\n",
       "      <td>123500 lb (56000 kg)</td>\n",
       "      <td>8495 ft square (789 m square)</td>\n",
       "      <td>14.5 lb / ft square (71 kg / m square)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ch - 53e super stallion</td>\n",
       "      <td>heavy - lift helicopter</td>\n",
       "      <td>73500 lb (33300 kg)</td>\n",
       "      <td>4900 ft square (460 m square)</td>\n",
       "      <td>15 lb / ft square (72 kg / m square)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  aircraft                    description  \\\n",
       "0          robinson r - 22       light utility helicopter   \n",
       "1     bell 206b3 jetranger  turboshaft utility helicopter   \n",
       "2         ch - 47d chinook        tandem rotor helicopter   \n",
       "3              mil mi - 26        heavy - lift helicopter   \n",
       "4  ch - 53e super stallion        heavy - lift helicopter   \n",
       "\n",
       "       max gross weight                total disk area  \\\n",
       "0      1370 lb (635 kg)  497 ft square (46.2 m square)   \n",
       "1     3200 lb (1451 kg)  872 ft square (81.1 m square)   \n",
       "2   50000 lb (22680 kg)  5655 ft square (526 m square)   \n",
       "3  123500 lb (56000 kg)  8495 ft square (789 m square)   \n",
       "4   73500 lb (33300 kg)  4900 ft square (460 m square)   \n",
       "\n",
       "                         max disk loading  \n",
       "0   2.6 lb / ft square (14 kg / m square)  \n",
       "1   3.7 lb / ft square (18 kg / m square)  \n",
       "2   8.8 lb / ft square (43 kg / m square)  \n",
       "3  14.5 lb / ft square (71 kg / m square)  \n",
       "4    15 lb / ft square (72 kg / m square)  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class FC(nn.Module):\n",
    "    def __init__(self, in_size, out_size, dropout_r=0., use_relu=True):\n",
    "        super(FC, self).__init__()\n",
    "        self.dropout_r = dropout_r\n",
    "        self.use_relu = use_relu\n",
    "\n",
    "        self.linear = nn.Linear(in_size, out_size)\n",
    "\n",
    "        if use_relu:\n",
    "            self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "        if dropout_r > 0:\n",
    "            self.dropout = nn.Dropout(dropout_r, inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "\n",
    "        if self.use_relu:\n",
    "            x = self.relu(x)\n",
    "\n",
    "        if self.dropout_r > 0:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, hidden_size, ff_size, dropout):\n",
    "        super(FFN, self).__init__()\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            in_size=hidden_size,\n",
    "            mid_size=ff_size,\n",
    "            out_size=hidden_size,\n",
    "            dropout_r=dropout,\n",
    "            use_relu=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_size, mid_size, out_size, dropout_r=0., use_relu=True):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.fc = FC(in_size, mid_size, dropout_r=dropout_r, use_relu=use_relu)\n",
    "        self.linear = nn.Linear(mid_size, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(self.fc(x))\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, size, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "        self.a_2 = nn.Parameter(torch.ones(size))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "    \n",
    "    \n",
    "class MHAtt(nn.Module):\n",
    "    def __init__(self, head_num, hidden_size, dropout, hidden_size_head):\n",
    "        super(MHAtt, self).__init__()\n",
    "        self.head_num = head_num\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_size_head = hidden_size_head\n",
    "        self.linear_v = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_k = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear_merge = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout, inplace=False)\n",
    "\n",
    "    def forward(self, v, k, q, mask):\n",
    "        n_batches = q.size(0)\n",
    "\n",
    "        v = self.linear_v(v).view(\n",
    "            n_batches,\n",
    "            -1,\n",
    "            self.head_num,\n",
    "            self.hidden_size_head\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        k = self.linear_k(k).view(\n",
    "            n_batches,\n",
    "            -1,\n",
    "            self.head_num,\n",
    "            self.hidden_size_head\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        q = self.linear_q(q).view(\n",
    "            n_batches,\n",
    "            -1,\n",
    "            self.head_num,\n",
    "            self.hidden_size_head\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        atted = self.att(v, k, q, mask)\n",
    "        atted = atted.transpose(1, 2).contiguous().view(\n",
    "            n_batches,\n",
    "            -1,\n",
    "            self.hidden_size\n",
    "        )\n",
    "\n",
    "        atted = self.linear_merge(atted)\n",
    "\n",
    "        return atted\n",
    "\n",
    "    def att(self, value, key, query, mask):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, -1e9)\n",
    "\n",
    "        att_map = F.softmax(scores, dim=-1)\n",
    "        att_map = self.dropout(att_map)\n",
    "\n",
    "        return torch.matmul(att_map, value)\n",
    "    \n",
    "\n",
    "class SA(nn.Module):\n",
    "    def __init__(self, hidden_size, head_num, ff_size, dropout, hidden_size_head):\n",
    "        super(SA, self).__init__()\n",
    "\n",
    "        self.mhatt = MHAtt(head_num, hidden_size, dropout, hidden_size_head)\n",
    "        self.ffn = FFN(hidden_size, ff_size, dropout)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout, inplace=False)\n",
    "        self.norm1 = LayerNorm(hidden_size)\n",
    "\n",
    "        self.dropout2 = nn.Dropout(dropout, inplace=False)\n",
    "        self.norm2 = LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        output = self.mhatt(x, x, x, x_mask)\n",
    "        dropout_output = self.dropout1(output)\n",
    "        x = self.norm1(x + dropout_output)\n",
    "\n",
    "        x = self.norm2(x + self.dropout2(\n",
    "            self.ffn(x)\n",
    "        ))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.nn import util\n",
    "\n",
    "class NumGNN(nn.Module):\n",
    "\n",
    "    def __init__(self, node_dim, iteration_steps=1):\n",
    "        super(NumGNN, self).__init__()\n",
    "\n",
    "        self.node_dim = node_dim\n",
    "        self.iteration_steps = iteration_steps\n",
    "\n",
    "        self._node_weight_fc = torch.nn.Linear(node_dim, 1, bias=True)\n",
    "        self._self_node_fc = torch.nn.Linear(node_dim, node_dim, bias=True)\n",
    "        \n",
    "        self._dd_node_fc_left = torch.nn.Linear(node_dim, node_dim, bias=False)\n",
    "        self._dd_node_fc_right = torch.nn.Linear(node_dim, node_dim, bias=False)\n",
    "\n",
    "    def forward(self, d_node, graph):\n",
    "        d_node_len = d_node.size(1)\n",
    "\n",
    "        diagmat = torch.diagflat(torch.ones(d_node.size(1), dtype=torch.long, device=d_node.device))\n",
    "        diagmat = diagmat.unsqueeze(0).expand(d_node.size(0), -1, -1)\n",
    "        dd_graph = 1 - diagmat\n",
    "        \n",
    "        dd_graph_left = dd_graph * graph[:, :d_node_len, :d_node_len]\n",
    "        dd_graph_right = dd_graph * (1 - graph[:, :d_node_len, :d_node_len])\n",
    "\n",
    "        d_node_neighbor_num = dd_graph_left.sum(-1) + dd_graph_right.sum(-1)\n",
    "        d_node_neighbor_num_mask = (d_node_neighbor_num >= 1).long()\n",
    "        d_node_neighbor_num = util.replace_masked_values(d_node_neighbor_num.float(), d_node_neighbor_num_mask, 1)\n",
    "        \n",
    "        for step in range(self.iteration_steps):\n",
    "            d_node_weight = torch.sigmoid(self._node_weight_fc(d_node)).squeeze(-1)            \n",
    "\n",
    "            self_d_node_info = self._self_node_fc(d_node)\n",
    "\n",
    "            dd_node_info_left = self._dd_node_fc_left(d_node)\n",
    "            \n",
    "            dd_node_weight = util.replace_masked_values(\n",
    "                    d_node_weight.unsqueeze(1).expand(-1, d_node_len, -1),\n",
    "                    dd_graph_left,\n",
    "                    0)\n",
    "            \n",
    "            dd_node_info_left = torch.matmul(dd_node_weight, dd_node_info_left)\n",
    "\n",
    "            dd_node_info_right = self._dd_node_fc_right(d_node)\n",
    "            \n",
    "            dd_node_weight = util.replace_masked_values(\n",
    "                    d_node_weight.unsqueeze(1).expand(-1, d_node_len, -1),\n",
    "                    dd_graph_right,\n",
    "                    0)\n",
    "            \n",
    "            dd_node_info_right = torch.matmul(dd_node_weight, dd_node_info_right)\n",
    "\n",
    "            agg_d_node_info = (dd_node_info_left + dd_node_info_right) / d_node_neighbor_num.unsqueeze(-1)\n",
    "\n",
    "            d_node = F.relu(self_d_node_info + agg_d_node_info)\n",
    "\n",
    "        return d_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention = SA(hidden_size=768, head_num=4, ff_size=768, dropout=0.1, hidden_size_head=768 // 4)\n",
    "self_attention.to(device)\n",
    "\n",
    "representation = self_attention(representation, None)\n",
    "\n",
    "def detect_number(string):\n",
    "    if '-' not in string:\n",
    "        for _ in string.split(' '):\n",
    "            try:\n",
    "                __ = float(_)\n",
    "                return __\n",
    "            except:\n",
    "                return string\n",
    "    else:\n",
    "        return string\n",
    "import scipy.stats as ss\n",
    "\n",
    "outputs = []\n",
    "idxs = []\n",
    "for i, col in enumerate(cols):\n",
    "    tmp = []\n",
    "    for _ in table[col]:\n",
    "        tmp.append(detect_number(_))\n",
    "    if all([isinstance(_, float) for _ in tmp]):\n",
    "        outputs.append(ss.rankdata(tmp))\n",
    "        idxs.append(i)\n",
    "\n",
    "number_rank = torch.FloatTensor(outputs)\n",
    "graph_mask = number_rank.unsqueeze(1).expand(3, number_rank.size(-1), -1) \\\n",
    "    < number_rank.unsqueeze(-1).expand(3, -1, number_rank.size(-1))\n",
    "graph = graph_mask.long().to(device)\n",
    "\n",
    "d_nodes = representation[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_nodes.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = NumGNN(768, 1)\n",
    "gnn.to(device)\n",
    "\n",
    "d_node = gnn(d_nodes, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totally 13182 table with 92283 statements for train\n",
      "totally 1696 table with 12792 statements for val\n",
      "totally 1695 table with 12779 statements for test\n",
      "totally 833 table with 4171 statements for simple_test\n",
      "totally 862 table with 8608 statements for complex_test\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas\n",
    "\n",
    "with open('../Table-Fact-Checking/data/train_id.json') as f:\n",
    "    train_ids = json.load(f)\n",
    "    \n",
    "with open('../Table-Fact-Checking/data/val_id.json') as f:\n",
    "    val_ids = json.load(f)\n",
    "    \n",
    "with open('../Table-Fact-Checking/data/test_id.json') as f:\n",
    "    test_ids = json.load(f)\n",
    "\n",
    "with open('../Table-Fact-Checking/data/simple_test_id.json') as f:\n",
    "    simple_test_ids = json.load(f)\n",
    "\n",
    "with open('../Table-Fact-Checking/data/complex_test_id.json') as f:\n",
    "    complex_test_ids = json.load(f)\n",
    "    \n",
    "def collect_cols(string):\n",
    "    semi_column = False\n",
    "    buf = ''\n",
    "    cols = []\n",
    "    for c in string:\n",
    "        if c == ';':\n",
    "            semi_column = True\n",
    "            buf = ''\n",
    "            continue\n",
    "        \n",
    "        if semi_column:\n",
    "            if c == '#':\n",
    "                semi_column = False\n",
    "                cols.extend(json.loads(buf))\n",
    "                buf = ''\n",
    "            else:\n",
    "                buf += c\n",
    "    \n",
    "    cols = [_ for _ in list(set(cols)) if _ >= 0]\n",
    "    return cols\n",
    "        \n",
    "with open('data/full_cleaned_aggressive.json') as f:\n",
    "    full_cleaned = json.load(f)\n",
    "    \n",
    "pattern = '#([^;]);[^#]#'\n",
    "\"\"\"\n",
    "for ids, split in zip([train_ids, val_ids, test_ids, simple_test_ids, complex_test_ids], \\\n",
    "                      ['train', 'val', 'test', 'simple_test', 'complex_test']):\n",
    "    examples = []\n",
    "    for t_id in ids:\n",
    "        table = pandas.read_csv('all_csv/{}'.format(t_id), '#')\n",
    "        columns = table.columns\n",
    "        \n",
    "        pair = full_cleaned[t_id]\n",
    "        for sent, label in zip(pair[0], pair[1]):\n",
    "            cols = collect_cols(sent)\n",
    "            if len(cols) == 0:\n",
    "                cols = [0, 1, 2]\n",
    "            else:\n",
    "                if 0 not in cols:\n",
    "                    cols.insert(0, 0)\n",
    "            \n",
    "            text = ''\n",
    "            for i in range(len(table)):\n",
    "                text += 'row {} is : '.format(i + 1)\n",
    "                entry = table.iloc[i]\n",
    "                for col in cols:\n",
    "                    text += '{} is {} , '.format(columns[col], entry[col])\n",
    "                \n",
    "                if i < len(table) - 1:\n",
    "                    text = text[:-2] + ' . '\n",
    "                else:\n",
    "                    text = text[:-2]\n",
    "            \n",
    "            sent = re.sub(r'#([^;]+);([^#]+)#', r'\\1', sent).lower()\n",
    "            \n",
    "            examples.append((t_id, text, pair[-1], sent, label))\n",
    "\n",
    "    with open('data/{}_baseline_examples.json'.format(split), 'w') as f:\n",
    "        json.dump(examples, f, indent=2)\n",
    "\"\"\"   \n",
    "\n",
    "for ids, split in zip([train_ids, val_ids, test_ids, simple_test_ids, complex_test_ids], \\\n",
    "                      ['train', 'val', 'test', 'simple_test', 'complex_test']):\n",
    "    sents = {}\n",
    "    for t_id in ids:\n",
    "        pair = full_cleaned[t_id]\n",
    "        for sent in pair[0]:\n",
    "            cols = collect_cols(sent)\n",
    "            if len(cols) == 0:\n",
    "                cols = [0, 1, 2]\n",
    "            else:\n",
    "                if 0 not in cols:\n",
    "                    cols.insert(0, 0)\n",
    "\n",
    "            sent = re.sub(r'#([^;]+);([^#]+)#', r'\\1', sent).lower()\n",
    "            if t_id not in sents:\n",
    "                sents[t_id] = [[sent], [cols], pair[1], pair[-1]]\n",
    "            else:\n",
    "                sents[t_id][0].append(sent)\n",
    "                sents[t_id][1].append(cols)\n",
    "\n",
    "    with open('data/{}_examples.json'.format(split), 'w') as f:\n",
    "        json.dump(sents, f, indent=2)\n",
    "    \n",
    "    total = sum([len(v[1]) for k, v in sents.items()])\n",
    "    print('totally {} table with {} statements for {}'.format(len(sents), total, split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       outcome               date                  location surface  \\\n",
      "0       winner         2 may 1999    coatzacoalcos , mexico    hard   \n",
      "1       winner       11 july 1999      felixstowe , england   grass   \n",
      "2  runner - up    6 february 2000  wellington , new zealand    hard   \n",
      "3  runner - up        28 may 2000     el paso , texas , usa    hard   \n",
      "4  runner - up  14 september 2003           spoleto , italy    clay   \n",
      "5       winner    6 february 2005  wellington , new zealand    hard   \n",
      "6       winner     28 august 2005              jesi , italy    hard   \n",
      "7       winner    5 february 2006       taupo , new zealand    hard   \n",
      "8       winner   12 february 2006  wellington , new zealand    hard   \n",
      "9       winner      20 april 2008         mazatlán , mexico    hard   \n",
      "\n",
      "     opponent in final                      score  \n",
      "0      candice jairala          3 - 6 6 - 3 7 - 5  \n",
      "1         karen nugent                6 - 4 6 - 4  \n",
      "2    mirielle dittmann  6 - 7 (5) 6 - 1 6 - 7 (5)  \n",
      "3        erin burdette                1 - 6 3 - 6  \n",
      "4      lenka snajdrova                4 - 6 3 - 6  \n",
      "5    mirielle dittmann          2 - 6 6 - 1 6 - 1  \n",
      "6        vanessa pinto            6 - 2 7 - 6 (6)  \n",
      "7     natsumi hamamura                6 - 1 6 - 2  \n",
      "8  katerina kramperová          6 - 4 1 - 6 6 - 0  \n",
      "9        anna lubinsky                6 - 2 6 - 1  \n",
      "outcome              object\n",
      "date                 object\n",
      "location             object\n",
      "surface              object\n",
      "opponent in final    object\n",
      "score                object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas\n",
    "\n",
    "with open('../Table-Fact-Checking/data/test_id.json') as f:\n",
    "    test_ids = json.load(f)\n",
    "\n",
    "for t_id in test_ids:\n",
    "    table = pandas.read_csv('all_csv/{}'.format(t_id), '#')\n",
    "    print(table)\n",
    "    columns = table.columns\n",
    "    print(table.dtypes)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
